/**
 * Multi-Provider Chat Service with Automatic Fallback
 *
 * Provider Priority Chain:
 * 1. Ollama kimi-k2:1t-cloud (Primary)
 * 2. Ollama qwen-coder:480b-cloud (Secondary)
 * 3. Google Gemini 2.0 Flash (Tertiary)
 * 4. OpenAI GPT-4o (Quaternary)
 *
 * Features:
 * - Automatic provider failover via HTTP API endpoints
 * - Feature-optimized routing (e.g., Gemini for web search)
 * - Conversation continuity across provider switches
 * - Detailed logging for debugging
 *
 * NOTE: This service makes HTTP requests to /api/unified/chat endpoint
 * It does NOT directly import provider SDKs (which would break client-side builds)
 */

import { AIProvider } from '../types';

export interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface SearchResult {
  title: string;
  snippet: string;
  link: string;
}

export interface ChatResponse {
  text: string;
  sources?: SearchResult[];
  provider?: AIProvider;
  model?: string;
}

/**
 * Chat class with automatic multi-provider fallback
 */
export class Chat {
  private messages: ChatMessage[] = [];
  private systemInstruction?: string;
  private temperature: number;
  private topP: number;
  private lastSuccessfulProvider?: AIProvider;

  constructor(
    systemInstruction?: string,
    options?: {
      temperature?: number;
      topP?: number;
    }
  ) {
    this.systemInstruction = systemInstruction;
    this.temperature = options?.temperature ?? 0.7;
    this.topP = options?.topP ?? 0.9;

    console.log('ğŸ¤– Multi-Provider Chat initialized');
    console.log(`ğŸ“Š Temperature: ${this.temperature}, Top-P: ${this.topP}`);
  }

  /**
   * Send a message with automatic provider fallback
   */
  async sendMessage(userMessage: string): Promise<string> {
    console.log(`ğŸ’¬ Sending message (${this.messages.length} messages in history)`);

    this.messages.push({
      role: 'user',
      content: userMessage,
    });

    try {
      const requestMessages: ChatMessage[] = [];

      if (this.systemInstruction) {
        requestMessages.push({
          role: 'system',
          content: this.systemInstruction,
        });
      }

      requestMessages.push(...this.messages);

      // Make HTTP request to unified API endpoint
      const response = await fetch('/api/unified/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          messages: requestMessages,
          temperature: this.temperature,
          topP: this.topP,
        }),
      });

      if (!response.ok) {
        throw new Error(`API error: ${response.status}`);
      }

      const data = await response.json();
      this.lastSuccessfulProvider = data.provider as AIProvider;

      this.messages.push({
        role: 'assistant',
        content: data.content,
      });

      console.log(`âœ… Response received from ${data.provider} (${data.model})`);

      return data.content;
    } catch (error) {
      console.error('âŒ All providers failed:', error);
      // Remove the user message since it failed
      this.messages.pop();
      throw error;
    }
  }

  /**
   * Analyze an image with automatic provider fallback
   * Priority: Gemini (best vision) â†’ GPT-4o â†’ Ollama (llava)
   */
  async analyzeImage(
    imageBase64: string,
    prompt: string
  ): Promise<string> {
    console.log('ğŸ–¼ï¸ Analyzing image with multi-provider fallback');

    try {
      // Make HTTP request to unified API endpoint
      const response = await fetch('/api/unified/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          messages: [
            {
              role: 'user',
              content: prompt,
              imageUrl: imageBase64,
            },
          ],
          temperature: this.temperature,
          topP: this.topP,
        }),
      });

      if (!response.ok) {
        throw new Error(`API error: ${response.status}`);
      }

      const data = await response.json();
      console.log(`âœ… Image analyzed by ${data.provider} (${data.model})`);

      return data.content;
    } catch (error) {
      console.error('âŒ Image analysis failed on all providers:', error);
      throw error;
    }
  }

  /**
   * Get conversation history
   */
  getHistory(): ChatMessage[] {
    return [...this.messages];
  }

  /**
   * Clear conversation history
   */
  clearHistory(): void {
    this.messages = [];
    console.log('ğŸ—‘ï¸ Conversation history cleared');
  }

  /**
   * Get the last successful provider
   */
  getLastProvider(): AIProvider | undefined {
    return this.lastSuccessfulProvider;
  }
}

/**
 * Perform a web search with automatic fallback
 * Prioritizes Gemini (has built-in web search with sources)
 */
export async function performSearch(
  query: string,
  options?: {
    temperature?: number;
    topP?: number;
    maxLength?: number;
  }
): Promise<ChatResponse> {
  const maxLength = options?.maxLength || 500;

  console.log('ğŸ” Performing search with multi-provider fallback');
  console.log(`ğŸ“ Max length: ${maxLength} words`);

  // Try Gemini first (has web search capability)
  try {
    console.log('ğŸ”„ Trying Gemini with web search (ä¼˜å…ˆé€‰æ‹©)');
    const response = await performSearchWithGemini(query, maxLength, options);
    console.log('âœ… Success with Gemini web search');
    return response;
  } catch (error) {
    console.log('âš ï¸ Gemini search failed, falling back to Ollama');
  }

  // Fallback to Ollama (knowledge-based, no web search)
  try {
    console.log('ğŸ”„ Trying Ollama (knowledge-based)');
    const response = await performSearchWithOllama(query, maxLength, options);
    console.log('âœ… Success with Ollama');
    return response;
  } catch (error) {
    console.log('âš ï¸ Ollama failed, trying GPT-4o');
  }

  // Final fallback to GPT-4o
  try {
    console.log('ğŸ”„ Trying GPT-4o');
    const response = await performSearchWithOpenAI(query, maxLength, options);
    console.log('âœ… Success with GPT-4o');
    return response;
  } catch (error) {
    throw new Error('Search failed on all providers');
  }
}

/**
 * Search with Gemini (has web search)
 * NOTE: This function is currently disabled client-side
 * The client should call /api/unified/chat directly instead
 */
async function performSearchWithGemini(
  query: string,
  maxLength: number,
  options?: { temperature?: number; topP?: number }
): Promise<ChatResponse> {
  // Use unified API endpoint instead of direct SDK import
  const enhancedQuery = `è«‹å›ç­”ä»¥ä¸‹å•é¡Œï¼ˆé™åˆ¶åœ¨ ${maxLength} å­—ä»¥å…§ï¼‰ï¼š

${query}

è«‹æä¾›æº–ç¢ºã€ç°¡æ½”çš„å›ç­”ï¼Œä¸¦åœ¨é©ç•¶æ™‚å¼•ç”¨ä¾†æºã€‚`;

  const response = await fetch('/api/unified/chat', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      messages: [{ role: 'user', content: enhancedQuery }],
      temperature: options?.temperature ?? 0.7,
      topP: options?.topP ?? 0.9,
      enableWebSearch: true,
    }),
  });

  if (!response.ok) {
    throw new Error(`API error: ${response.status}`);
  }

  const data = await response.json();

  return {
    text: data.content,
    sources: data.groundingSources,
    provider: AIProvider.GEMINI,
    model: data.model,
  };
}

/**
 * Search with Ollama (knowledge-based, no web search)
 */
async function performSearchWithOllama(
  query: string,
  maxLength: number,
  options?: { temperature?: number; topP?: number }
): Promise<ChatResponse> {
  const enhancedQuery = `è«‹å›ç­”ä»¥ä¸‹å•é¡Œï¼ˆé™åˆ¶åœ¨ ${maxLength} å­—ä»¥å…§ï¼‰ï¼š

${query}

æ³¨æ„ï¼šè«‹åŸºæ–¼ä½ çš„çŸ¥è­˜æä¾›æº–ç¢ºçš„å›ç­”ã€‚å¦‚æœä¸ç¢ºå®šï¼Œè«‹æ˜ç¢ºèªªæ˜ã€‚`;

  const response = await fetch('/api/unified/chat', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      messages: [{ role: 'user', content: enhancedQuery }],
      temperature: options?.temperature ?? 0.7,
      topP: options?.topP ?? 0.9,
      model: 'kimi-k2:1t-cloud',
    }),
  });

  if (!response.ok) {
    throw new Error(`API error: ${response.status}`);
  }

  const data = await response.json();

  return {
    text: data.content,
    provider: data.provider as AIProvider,
    model: data.model,
  };
}

/**
 * Search with OpenAI GPT-4o
 */
async function performSearchWithOpenAI(
  query: string,
  maxLength: number,
  options?: { temperature?: number; topP?: number }
): Promise<ChatResponse> {
  const enhancedQuery = `è«‹å›ç­”ä»¥ä¸‹å•é¡Œï¼ˆé™åˆ¶åœ¨ ${maxLength} å­—ä»¥å…§ï¼‰ï¼š

${query}

è«‹æä¾›æº–ç¢ºã€ç°¡æ½”çš„å›ç­”ã€‚`;

  const response = await fetch('/api/unified/chat', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      messages: [{ role: 'user', content: enhancedQuery }],
      temperature: options?.temperature ?? 0.7,
      topP: options?.topP ?? 0.9,
    }),
  });

  if (!response.ok) {
    throw new Error(`API error: ${response.status}`);
  }

  const data = await response.json();

  return {
    text: data.content,
    provider: data.provider as AIProvider,
    model: data.model,
  };
}

/**
 * Analyze text with automatic fallback
 */
export async function analyzeText(
  prompt: string,
  options?: {
    temperature?: number;
    topP?: number;
    systemInstruction?: string;
  }
): Promise<ChatResponse> {
  console.log('ğŸ“ Analyzing text with multi-provider fallback');

  try {
    const messages: ChatMessage[] = [];

    if (options?.systemInstruction) {
      messages.push({
        role: 'system',
        content: options.systemInstruction,
      });
    }

    messages.push({
      role: 'user',
      content: prompt,
    });

    const response = await fetch('/api/unified/chat', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        messages,
        temperature: options?.temperature ?? 0.7,
        topP: options?.topP ?? 0.9,
      }),
    });

    if (!response.ok) {
      throw new Error(`API error: ${response.status}`);
    }

    const data = await response.json();

    console.log(`âœ… Text analyzed by ${data.provider} (${data.model})`);

    return {
      text: data.content,
      provider: data.provider as AIProvider,
      model: data.model,
    };
  } catch (error) {
    console.error('âŒ Text analysis failed on all providers:', error);
    throw error;
  }
}

/**
 * Analyze an image (standalone function)
 */
export async function analyzeImage(
  imageBase64: string,
  prompt: string,
  options?: {
    temperature?: number;
    topP?: number;
  }
): Promise<ChatResponse> {
  console.log('ğŸ–¼ï¸ Analyzing image (standalone)');

  try {
    const response = await fetch('/api/unified/chat', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        messages: [
          {
            role: 'user',
            content: prompt,
            imageUrl: imageBase64,
          },
        ],
        temperature: options?.temperature ?? 0.7,
        topP: options?.topP ?? 0.9,
      }),
    });

    if (!response.ok) {
      throw new Error(`API error: ${response.status}`);
    }

    const data = await response.json();

    return {
      text: data.content,
      provider: data.provider as AIProvider,
      model: data.model,
    };
  } catch (error) {
    console.error('âŒ Image analysis failed:', error);
    throw error;
  }
}

/**
 * Check which providers are currently available
 */
export async function checkAvailableProviders(): Promise<{
  ollama: boolean;
  gemini: boolean;
  openai: boolean;
}> {
  // Simple check - returns whether API keys are configured
  return {
    ollama: Boolean(process.env.OLLAMA_API_KEY),
    gemini: Boolean(process.env.GEMINI_API_KEY || process.env.API_KEY),
    openai: Boolean(process.env.OPENAI_API_KEY),
  };
}

/**
 * Get provider status with detailed information
 */
export async function getProviderStatus(): Promise<{
  primary: { name: string; available: boolean; model: string };
  secondary: { name: string; available: boolean; model: string };
  tertiary: { name: string; available: boolean; model: string };
  quaternary: { name: string; available: boolean; model: string };
}> {
  const available = await checkAvailableProviders();

  return {
    primary: {
      name: 'Ollama (Kimi K2)',
      available: available.ollama,
      model: 'kimi-k2:1t-cloud',
    },
    secondary: {
      name: 'Ollama (Qwen Coder)',
      available: available.ollama,
      model: 'qwen-coder:480b-cloud',
    },
    tertiary: {
      name: 'Google Gemini',
      available: available.gemini,
      model: 'gemini-2.0-flash-exp',
    },
    quaternary: {
      name: 'OpenAI GPT-4o',
      available: available.openai,
      model: 'gpt-4o',
    },
  };
}

// Export default for backward compatibility
export default {
  Chat,
  performSearch,
  analyzeText,
  analyzeImage,
  checkAvailableProviders,
  getProviderStatus,
};
