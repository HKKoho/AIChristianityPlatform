/**
 * Theology Assistant Service
 * Handles AI chat interactions for the Theology Assistant feature
 */

export interface ChatRequest {
  model: string;
  messages: Array<{ role: 'user' | 'assistant'; content: string }>;
  temperature: number;
  topP: number;
}

export interface ChatResponse {
  content: string;
  model: string;
}

/**
 * Call Ollama Cloud API for chat completions
 * Uses unified /api/chat endpoint (works in both dev and production)
 */
async function callOllamaCloud(request: ChatRequest): Promise<ChatResponse> {
  try {
    // Use unified API endpoint that works in both dev (Vite proxy) and production (Vercel)
    const response = await fetch('/api/chat', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        provider: 'ollama',
        model: request.model,
        messages: request.messages,
        temperature: request.temperature,
        topP: request.topP,
        maxTokens: 2000
      })
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Ollama Cloud API error (${response.status}): ${errorText}`);
    }

    const data = await response.json();

    // The unified endpoint returns { content, model, provider, usage }
    if (data.content) {
      return {
        content: data.content,
        model: data.model
      };
    }

    throw new Error('Invalid response format from Ollama Cloud API');
  } catch (error: any) {
    console.error('Ollama Cloud API error:', error);
    throw new Error(`Failed to get response: ${error.message}`);
  }
}

/**
 * Call OpenAI API for GPT models
 * Uses unified /api/chat endpoint (works in both dev and production)
 */
async function callOpenAI(request: ChatRequest): Promise<ChatResponse> {
  try {
    // Use unified API endpoint that works in both dev (Vite proxy) and production (Vercel)
    const response = await fetch('/api/chat', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        provider: 'openai',
        model: request.model,
        messages: request.messages,
        temperature: request.temperature,
        topP: request.topP,
        maxTokens: 2000
      })
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`OpenAI API error (${response.status}): ${errorText}`);
    }

    const data = await response.json();

    // The unified endpoint returns { content, model, provider, usage }
    if (data.content) {
      return {
        content: data.content,
        model: data.model
      };
    }

    throw new Error('Invalid response format from OpenAI API');
  } catch (error: any) {
    console.error('OpenAI API error:', error);
    throw new Error(`Failed to get response from OpenAI: ${error.message}`);
  }
}

/**
 * Call Google Gemini API with automatic fallback to Ollama
 * Uses unified endpoint with multi-provider failover
 */
async function callGemini(request: ChatRequest): Promise<ChatResponse> {
  try {
    console.log('ğŸ”„ Using multi-provider endpoint with automatic failover...');

    // Use unified API endpoint with automatic provider fallback
    // Priority: Ollama (kimi-k2:1t-cloud) â†’ Ollama (qwen-coder:480b-cloud) â†’ Gemini â†’ GPT-4o
    const response = await fetch('/api/unified/chat', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        messages: request.messages,
        temperature: request.temperature,
        topP: request.topP,
        maxTokens: 2000
      })
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Multi-provider API error (${response.status}): ${errorText}`);
    }

    const data = await response.json();

    if (data.content) {
      console.log(`âœ… Response received from ${data.provider} (${data.model})`);
      return {
        content: data.content,
        model: data.model || request.model
      };
    }

    throw new Error('Invalid response format from multi-provider API');
  } catch (error: any) {
    console.error('Multi-provider API error:', error);
    throw new Error(`Failed to get response: ${error.message}`);
  }
}

/**
 * Mock call for local Ollama models
 */
async function callLocalOllama(request: ChatRequest): Promise<ChatResponse> {
  const LOCAL_OLLAMA_URL = 'http://localhost:11434';

  try {
    const response = await fetch(`${LOCAL_OLLAMA_URL}/api/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: request.model,
        messages: request.messages,
        stream: false,
        options: {
          temperature: request.temperature,
          top_p: request.topP,
          num_predict: 2000
        }
      })
    });

    if (!response.ok) {
      throw new Error(`Local Ollama error: ${response.status}`);
    }

    const data = await response.json();

    if (data.message && data.message.content) {
      return {
        content: data.message.content,
        model: request.model
      };
    }

    throw new Error('Invalid response from local Ollama');
  } catch (error: any) {
    console.error('Local Ollama error:', error);
    throw new Error(`ç„¡æ³•é€£æ¥åˆ°æœ¬åœ° Ollama æœå‹™ã€‚è«‹ç¢ºä¿ï¼š\n1. Ollama å·²å®‰è£ä¸¦æ­£åœ¨é‹è¡Œ\n2. æ¨¡å‹å·²ä¸‹è¼‰ (ollama pull ${request.model})\n3. æœå‹™é‹è¡Œåœ¨ http://localhost:11434`);
  }
}

// Define which models should use Ollama Cloud (actual models in your account)
const OLLAMA_CLOUD_MODELS = [
  'kimi-k2:1t',
  'qwen3-coder:480b',
  'deepseek-v3.1:671b',
  'gpt-oss:120b',
  'gpt-oss:20b'
];

/**
 * Main chat function - routes to appropriate service based on model
 */
export async function sendChatMessage(request: ChatRequest): Promise<ChatResponse> {
  const model = request.model;

  // Determine which service to use based on model ID
  if (OLLAMA_CLOUD_MODELS.includes(model)) {
    // Ollama Cloud models (standard model names that are available in cloud)
    return callOllamaCloud(request);
  } else if (model.startsWith('gpt-')) {
    // OpenAI GPT models
    return callOpenAI(request);
  } else if (model.startsWith('gemini-')) {
    // Google Gemini models
    return callGemini(request);
  } else {
    // Local Ollama models
    return callLocalOllama(request);
  }
}

/**
 * Create system prompt for theology context
 */
export function createTheologySystemPrompt(mode: string): string {
  const basePrompt = `ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç¥å­¸ç ”ç©¶åŠ©æ‰‹ï¼Œæ“æœ‰æ·±åšçš„è–ç¶“çŸ¥è­˜ã€æ•™æœƒæ­·å²å’Œç³»çµ±ç¥å­¸ç†è§£ã€‚ä½ çš„å›æ‡‰æ‡‰è©²ï¼š

1. åŸºæ–¼è–ç¶“çœŸç†å’Œæ­£çµ±ç¥å­¸å‚³çµ±
2. æä¾›æº–ç¢ºçš„ç¶“æ–‡å¼•ç”¨å’Œæ­·å²èƒŒæ™¯
3. ä»¥å­¸è¡“åš´è¬¹ä½†æ˜“æ–¼ç†è§£çš„æ–¹å¼è¡¨é”
4. å°Šé‡ä¸åŒçš„ç¥å­¸ç«‹å ´ï¼Œä½†æ˜ç¢ºæŒ‡å‡ºä½ çš„è§€é»åŸºç¤
5. é¼“å‹µæ·±å…¥æ€è€ƒå’Œå±¬éˆæˆé•·

è«‹ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰ã€‚`;

  const modeSpecific = {
    'Theology Chat': 'ç•¶å‰æ¨¡å¼ï¼šç¥å­¸å°è©±ã€‚è«‹é‡å°ç”¨æˆ¶çš„ç¥å­¸å•é¡Œæä¾›æ·±å…¥ä¸”å¹³è¡¡çš„å›ç­”ã€‚',
    'Reading Q&A': 'ç•¶å‰æ¨¡å¼ï¼šæ–‡æª”å•ç­”ã€‚è«‹åŸºæ–¼å·²ä¸Šå‚³çš„æ–‡æª”å…§å®¹å›ç­”å•é¡Œï¼Œä¸¦æä¾›ç›¸é—œçš„å¼•ç”¨å’Œåˆ†æã€‚',
    'Assignment Assistant': 'ç•¶å‰æ¨¡å¼ï¼šä½œæ¥­åŠ©æ‰‹ã€‚è«‹å¹«åŠ©ç”¨æˆ¶å®Œæˆç¥å­¸ä½œæ¥­ï¼Œæä¾›å­¸è¡“æ€§çš„æŒ‡å°å’Œå»ºè­°ã€‚',
    'Resource Search': 'ç•¶å‰æ¨¡å¼ï¼šè³‡æºæœå°‹ã€‚è«‹å¹«åŠ©ç”¨æˆ¶æ‰¾åˆ°ç›¸é—œçš„ç¥å­¸è³‡æºå’Œåƒè€ƒæ–‡ç»ã€‚'
  };

  return `${basePrompt}\n\n${modeSpecific[mode as keyof typeof modeSpecific] || modeSpecific['Theology Chat']}`;
}
